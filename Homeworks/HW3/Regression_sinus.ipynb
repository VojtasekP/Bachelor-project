{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c815625-6aae-4f53-9602-f0f3647aabb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "from icecream import ic\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1edde2dd-3c51-47eb-ae44-47da774f1863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HW: \n",
    "#1) better looking code\n",
    "#2) working program\n",
    "#3) data generator(pytorch Dataset, DataLoader):\n",
    "#     sinusoids with different frequentions ~ U(1,10) - generate from 0 to 1000 (step = 1/0.1/0.01) \n",
    "#4) create NN predicting frequentions (regression) \n",
    "#5) plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80fa0aca-d7bf-40f5-904f-6eff38d753d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f11623fcc70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e78382ec-1ca6-47a7-8dcc-bf6bb8b5e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO try primitive convoltional and inceptiontime\n",
    "# TODO comment everything \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fbe85a9-1861-472c-9c30-d8600e8aeda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7261b54-f7ec-4e6d-8efa-53e6e0c87976",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_count, device='cpu'):\n",
    "        self.freq = 10*torch.rand(size=(data_count, 1), device=DEVICE) + 1\n",
    "        \n",
    "        random_uniform_shift = 10*torch.rand(size=(data_count, 1), device=DEVICE)\n",
    "        random_normal_shift = torch.normal(0, 1, size=(1, 1), device=DEVICE)\n",
    "        \n",
    "        self.points = torch.arange(0,6,1/40, device=DEVICE).repeat(data_count, 1)  + random_uniform_shift\n",
    "        self.phase = torch.normal(0, 5, size=(data_count, 1), device=DEVICE)\n",
    "        self.amplitude = (self.points[0] - random_uniform_shift[0])**2 \n",
    "        self.data_matrix = torch.sin(self.points * self.freq + self.phase)\n",
    "        \n",
    "        for i in range(data_count):\n",
    "            if random.random() < 0.3:\n",
    "                self.amplitude = torch.flip(self.amplitude, dims=(-1,))\n",
    "                random_noise = (self.amplitude+1)*torch.normal(0, 1, size=(1, len(self.data_matrix[0])), device=DEVICE)\n",
    "                self.data_matrix[i] = self.amplitude*self.data_matrix[i] + random_noise\n",
    "            elif 0.3<=random.random() <0.7:\n",
    "                random_noise = (self.amplitude+1)*torch.normal(0, 1, size=(1, len(self.data_matrix[0])), device=DEVICE)\n",
    "                self.data_matrix[i] = self.amplitude*self.data_matrix[i] + random_noise               \n",
    "            else:\n",
    "                random_noise = torch.normal(0, 1, size=(1, len(self.data_matrix[0])), device=DEVICE)\n",
    "                self.data_matrix[i] = self.data_matrix[i] + random_noise\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.freq)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_matrix[idx], self.freq[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdb17abe-3038-4512-ad7a-854f27c44dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, layers: list[dict]):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9262b6fc-0559-499f-a0b8-f359a1ccec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, conv_layers: list[dict], fullycon_layers: list [dict]):  # todo: repair\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.ModuleList(conv_layers)\n",
    "        self.fullycon_layers = nn.ModuleList(fullycon_layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        for layer in self.conv_layers:\n",
    "            x = F.relu(layer(x))\n",
    "            \n",
    "        x = torch.flatten(x, -1)\n",
    "        \n",
    "        for layer in self.fullycon_layers:\n",
    "            x = F.relu(layer(x))\n",
    "        print(x.shape)   \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbd57dcf-c770-4710-beb9-93293dad50cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuroNet:\n",
    "    def __init__(self, control_center: str, meta: dict = ''):\n",
    "        self._load_yaml(control_center)\n",
    "        self.model = self.build_model()\n",
    "        self.model.to('cuda')\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, self.training_params[\"epoch_num\"])\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.history = []\n",
    "    def _load_yaml(self, yaml_path:Path) -> None:\n",
    "        \n",
    "        path = Path(yaml_path) \n",
    "        with path.open(mode = \"r\") as yaml_file:\n",
    "            data = yaml.load(yaml_file, Loader = yaml.SafeLoader)\n",
    "            \n",
    "         \n",
    "        self.training_params=data[\"training_params\"]\n",
    "        self.eval_params=data[\"eval_params\"]\n",
    "        self.lr = data[\"lr\"]\n",
    "        \n",
    "  \n",
    "    # load convolutional layers\n",
    "        self.conv_layers_config = data['conv_layers']\n",
    "        self.conv_layers = []\n",
    "        for layer_config in self.conv_layers_config:\n",
    "            self.conv_layers.append(eval('nn.' + layer_config['name'])(*layer_config.get('args',[]), **layer_config.get('kwargs', {})))\n",
    "    # load fully connected layers    \n",
    "        self.fullycon_layers_config = data['fullycon_layers']\n",
    "        self.fullycon_layers = []\n",
    "        for layer_config in self.fullycon_layers_config:\n",
    "            self.fullycon_layers.append(eval('nn.' + layer_config['name'])(*layer_config.get('args',[]), **layer_config.get('kwargs', {})))\n",
    "            \n",
    "    def build_model(self):\n",
    "        return NN(self.fullycon_layers) \n",
    "        \n",
    "    def train_model(self, training_data: Dataset, testing_data: Dataset):\n",
    "\n",
    "        step = 0\n",
    "\n",
    "        train_dataloader = DataLoader(training_data, **self.training_params.get(\"dataloader_params\", {}))\n",
    "        test_dataloader = DataLoader(testing_data, **self.eval_params)\n",
    "\n",
    "        for epoch in range(self.training_params[\"epoch_num\"]):\n",
    "            self.model.train()\n",
    "\n",
    "            for batch_id, (inputs, targets) in enumerate(\n",
    "                    tqdm(train_dataloader, desc=f'epoch {epoch + 1}/ {self.training_params[\"epoch_num\"]} ')):\n",
    "                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                writer.add_scalar('Training Loss', loss, global_step=step)\n",
    "                step += 1\n",
    "\n",
    "            self.model.eval()  # TODO make validation loss\n",
    "            for batch_id, (inputs, targets) in enumerate(test_dataloader):\n",
    "                outputs = self.model(inputs)\n",
    "                mse = np.mean((outputs.detach().cpu().numpy() - targets.detach().cpu().numpy()) ** 2)\n",
    "                # print(mse)\n",
    "                writer.add_scalar('mean square error', mse, global_step=epoch)\n",
    "                self.history.append(mse)\n",
    "\n",
    "            ic(self.scheduler.get_last_lr())\n",
    "            last_lr = self.scheduler.get_last_lr()[0]  # WHY IS IT A LIST\n",
    "            writer.add_scalar('learning rate', last_lr, global_step=epoch)\n",
    "            self.scheduler.step()\n",
    "        writer.close()\n",
    "        \n",
    "    def predict(self, points: torch.Tensor):\n",
    "        return self.model(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffde3738-8faf-46d9-9e59-5402f9639a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SineDataset(128*128, DEVICE)\n",
    "train_data, test_data = random_split(dataset, [100*128,28*128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25e4e0f4-f56b-47f1-9161-bd143835d35c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'conv_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m neuro_net\u001b[38;5;241m=\u001b[39m \u001b[43mNeuroNet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMLP.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m, in \u001b[0;36mNeuroNet.__init__\u001b[0;34m(self, control_center, meta)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, control_center: \u001b[38;5;28mstr\u001b[39m, meta: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_yaml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontrol_center\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_model()\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 23\u001b[0m, in \u001b[0;36mNeuroNet._load_yaml\u001b[0;34m(self, yaml_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# load convolutional layers\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layers_config \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconv_layers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer_config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layers_config:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'conv_layers'"
     ]
    }
   ],
   "source": [
    "neuro_net= NeuroNet('MLP.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f20e1d90-ec6b-43be-bcb8-3f6c291e0501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1/ 10 :   0%|                                                                                                                                                                                                                                               | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 8, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mneuro_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 51\u001b[0m, in \u001b[0;36mNeuroNet.train_model\u001b[0;34m(self, training_data, testing_data)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     50\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(inputs)\n\u001b[0;32m---> 51\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/t2/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/t2/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/t2/lib/python3.11/site-packages/torch/nn/modules/loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/t2/lib/python3.11/site-packages/torch/nn/functional.py:3328\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3326\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3328\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/anaconda3/envs/t2/lib/python3.11/site-packages/torch/functional.py:73\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "neuro_net.train_model(train_data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e7ffc5-4c5a-4246-a47f-a0d1a39d0387",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"mean square error: {neuro_net.history[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5aa6aa-0005-4ae2-a413-d831353e4de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565af639-180a-4e7f-8875-9f3838427c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(neuro_net.history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9214354f-3532-476f-861b-e30ebc91c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    points, freq = test_data[i]\n",
    "    prediction = neuro_net.predict(points)\n",
    "    print(f'Actuall freq: {freq}, Predicted freq: {prediction}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8af209d-d1d6-4e58-9331-b7860c5b55a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.plot(dataset.points[i].cpu(), dataset.data_matrix[i].cpu())\n",
    "    # TODO original sine waves without noise with \n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef8d067-1d6b-402a-8168-745852a3a75d",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Validation loss\n",
    "- random shifts\n",
    "- random noise(rand float*constant), plot graph (constant/validation_loss)\n",
    "- random aplitudes(rand int)\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
